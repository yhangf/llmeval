#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨è¯„ä¼°å™¨æ¨¡å—
å¤„ç†éç¼–ç¨‹é¢˜çš„é€šç”¨è¯„ä¼°é€»è¾‘
"""

import asyncio
from typing import Dict, Any
from .evaluation_types import EvaluationScores, EvaluationResult
from .score_calculator import ScoreCalculator
from .text_analyzer import TextAnalyzer


class GeneralEvaluator:
    """é€šç”¨è¯„ä¼°å™¨"""
    
    def __init__(self, prompt_loader):
        self.prompt_loader = prompt_loader
        self.score_calculator = ScoreCalculator()
        self.text_analyzer = TextAnalyzer()
    
    async def evaluate_general_response(self, question_text: str, model_answer: str, 
                                      reference: str, evaluator_model, logger=None) -> Dict[str, Any]:
        """ä½¿ç”¨è¯„ä¼°æ¨¡å‹è¯„ä¼°é€šç”¨å›ç­”"""
        print(f"ğŸ¯ ä½¿ç”¨é€šç”¨è¯„ä¼°æ¨¡æ¿")
        
        evaluation = {
            "scores": {},
            "feedback": "",
            "details": {},
            "evaluation_tokens": 0
        }
        
        try:
            # å¹¶å‘è¯„ä¼°ä¸‰ä¸ªç»´åº¦
            accuracy_task = self.evaluate_accuracy_with_model(
                question_text, model_answer, reference, evaluator_model, logger
            )
            completeness_task = self.evaluate_completeness_with_model(
                question_text, model_answer, reference, evaluator_model, logger
            )
            clarity_task = self.evaluate_clarity_with_model(
                question_text, model_answer, evaluator_model, logger
            )
            
            # ç­‰å¾…æ‰€æœ‰è¯„ä¼°å®Œæˆ
            accuracy_score, completeness_score, clarity_score = await asyncio.gather(
                accuracy_task, completeness_task, clarity_task
            )
            
            evaluation["scores"]["accuracy"] = accuracy_score["score"]
            evaluation["scores"]["completeness"] = completeness_score["score"]
            evaluation["scores"]["clarity"] = clarity_score["score"]
            
            # è®¡ç®—æ€»åˆ†
            scores = evaluation["scores"]
            evaluation["scores"]["overall"] = self.score_calculator.calculate_overall_score(
                EvaluationScores(
                    accuracy=scores["accuracy"],
                    completeness=scores["completeness"],
                    clarity=scores["clarity"]
                )
            )
            
            # ç»Ÿè®¡è¯„ä¼°æ¨¡å‹ä½¿ç”¨çš„token
            evaluation["evaluation_tokens"] = (
                accuracy_score["tokens"] + 
                completeness_score["tokens"] + 
                clarity_score["tokens"]
            )
            
            # ç”Ÿæˆåé¦ˆ
            evaluation["feedback"] = self._generate_feedback(evaluation["scores"], model_answer)
            
        except Exception as e:
            print(f"è¯„ä¼°æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
            # å¦‚æœè¯„ä¼°æ¨¡å‹å¤±è´¥ï¼Œå›é€€åˆ°ç¨‹åºè®¡ç®—
            evaluation = await self._fallback_evaluation(model_answer, reference, question_text)
        
        return evaluation
    
    async def evaluate_accuracy_with_model(self, question: str, answer: str, 
                                         reference: str, evaluator_model, logger=None) -> Dict[str, Any]:
        """ä½¿ç”¨è¯„ä¼°æ¨¡å‹è¯„ä¼°å‡†ç¡®æ€§"""
        prompt = self.prompt_loader.create_evaluation_prompt(
            question, answer, reference, "accuracy"
        )
        
        try:
            await asyncio.sleep(1)  # é¿å…APIé™åˆ¶
            
            # è®°å½•è¯„ä¼°æ¨¡å‹è¯·æ±‚
            if logger:
                logger.log_model_request(evaluator_model.__class__.__name__, prompt, 
                                       {"max_tokens": 50, "temperature": 0.1})
            
            response = await evaluator_model.generate(prompt, max_tokens=50, temperature=0.1)
            
            if response.get('error'):
                raise Exception(response['error'])
            
            content = response.get('content', '').strip()
            print(f"å‡†ç¡®æ€§è¯„ä¼°ç»“æœ: {content}")
            
            # è®°å½•è¯„ä¼°æ¨¡å‹å›ç­”
            if logger:
                logger.log_model_response(evaluator_model.__class__.__name__, response, "å‡†ç¡®æ€§è¯„ä¼°")
            
            # ä»å›ç­”ä¸­æå–åˆ†æ•°
            score = self.score_calculator.extract_score_from_response(content)
            
            return {
                "score": score,
                "tokens": response.get('tokens_used', 0),
                "raw_response": content
            }
            
        except Exception as e:
            print(f"å‡†ç¡®æ€§è¯„ä¼°å¤±è´¥: {str(e)}")
            raise e
    
    async def evaluate_completeness_with_model(self, question: str, answer: str, 
                                             reference: str, evaluator_model, logger=None) -> Dict[str, Any]:
        """ä½¿ç”¨è¯„ä¼°æ¨¡å‹è¯„ä¼°å®Œæ•´æ€§"""
        prompt = self.prompt_loader.create_evaluation_prompt(
            question, answer, reference, "completeness"
        )
        
        try:
            await asyncio.sleep(1)  # é¿å…APIé™åˆ¶
            
            # è®°å½•è¯„ä¼°æ¨¡å‹è¯·æ±‚
            if logger:
                logger.log_model_request(evaluator_model.__class__.__name__, prompt, 
                                       {"max_tokens": 50, "temperature": 0.1})
            
            response = await evaluator_model.generate(prompt, max_tokens=5000, temperature=0.4)
            
            if response.get('error'):
                raise Exception(response['error'])
            
            content = response.get('content', '').strip()
            print(f"å®Œæ•´æ€§è¯„ä¼°ç»“æœ: {content}")
            
            # è®°å½•è¯„ä¼°æ¨¡å‹å›ç­”
            if logger:
                logger.log_model_response(evaluator_model.__class__.__name__, response, "å®Œæ•´æ€§è¯„ä¼°")
            
            # ä»å›ç­”ä¸­æå–åˆ†æ•°
            score = self.score_calculator.extract_score_from_response(content)
            
            return {
                "score": score,
                "tokens": response.get('tokens_used', 0),
                "raw_response": content
            }
            
        except Exception as e:
            print(f"å®Œæ•´æ€§è¯„ä¼°å¤±è´¥: {str(e)}")
            raise e
    
    async def evaluate_clarity_with_model(self, question: str, answer: str, 
                                        evaluator_model, logger=None) -> Dict[str, Any]:
        """ä½¿ç”¨è¯„ä¼°æ¨¡å‹è¯„ä¼°æ¸…æ™°åº¦"""
        prompt = self.prompt_loader.create_evaluation_prompt(
            question, answer, "", "clarity"
        )
        
        try:
            await asyncio.sleep(1)  # é¿å…APIé™åˆ¶
            
            # è®°å½•è¯„ä¼°æ¨¡å‹è¯·æ±‚
            if logger:
                logger.log_model_request(evaluator_model.__class__.__name__, prompt, 
                                       {"max_tokens": 50, "temperature": 0.1})
            
            response = await evaluator_model.generate(prompt, max_tokens=50, temperature=0.1)
            
            if response.get('error'):
                raise Exception(response['error'])
            
            content = response.get('content', '').strip()
            print(f"æ¸…æ™°åº¦è¯„ä¼°ç»“æœ: {content}")
            
            # è®°å½•è¯„ä¼°æ¨¡å‹å›ç­”
            if logger:
                logger.log_model_response(evaluator_model.__class__.__name__, response, "æ¸…æ™°åº¦è¯„ä¼°")
            
            # ä»å›ç­”ä¸­æå–åˆ†æ•°
            score = self.score_calculator.extract_score_from_response(content)
            
            return {
                "score": score,
                "tokens": response.get('tokens_used', 0),
                "raw_response": content
            }
            
        except Exception as e:
            print(f"æ¸…æ™°åº¦è¯„ä¼°å¤±è´¥: {str(e)}")
            raise e
    
    async def _fallback_evaluation(self, model_answer: str, reference: str, question_text: str) -> Dict[str, Any]:
        """å¤‡ç”¨è¯„ä¼°æ–¹æ³•"""
        question = {"content": question_text, "category": ""}
        
        accuracy = self._evaluate_accuracy_fallback(model_answer, reference, question)
        completeness = self._evaluate_completeness_fallback(model_answer, reference, question)
        clarity = self._evaluate_clarity_fallback(model_answer, question)
        
        scores = EvaluationScores(accuracy=accuracy, completeness=completeness, clarity=clarity)
        overall = self.score_calculator.calculate_overall_score(scores)
        
        return {
            "scores": {
                "accuracy": accuracy,
                "completeness": completeness,
                "clarity": clarity,
                "overall": overall
            },
            "feedback": "ä½¿ç”¨å¤‡ç”¨è¯„ä¼°æ–¹æ³•è¿›è¡Œè¯„ä¼°",
            "details": {
                "answer_length": len(model_answer),
                "reference_length": len(reference),
                "keyword_match": self.text_analyzer.calculate_keyword_match(model_answer, reference),
                "structure_score": self.text_analyzer.evaluate_structure(model_answer)
            },
            "evaluation_tokens": 0
        }
    
    def _evaluate_accuracy_fallback(self, model_answer: str, reference: str, question: Dict) -> float:
        """è¯„ä¼°ç­”æ¡ˆå‡†ç¡®æ€§ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        if not model_answer or not reference:
            return 0.0
        
        # å…³é”®è¯åŒ¹é…
        keyword_score = self.text_analyzer.calculate_keyword_match(model_answer, reference)
        
        # è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        semantic_score = self.text_analyzer.calculate_semantic_similarity(model_answer, reference)
        
        # ç‰¹å®šé¢†åŸŸè¯„ä¼°
        domain_score = self.text_analyzer.evaluate_domain_specific(model_answer, question)
        
        return min(100.0, (keyword_score + semantic_score + domain_score) / 3)
    
    def _evaluate_completeness_fallback(self, model_answer: str, reference: str, question: Dict) -> float:
        """è¯„ä¼°ç­”æ¡ˆå®Œæ•´æ€§ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        if not model_answer:
            return 0.0
        
        # é•¿åº¦æ¯”è¾ƒ
        length_ratio = min(1.0, len(model_answer) / max(len(reference), 100))
        length_score = length_ratio * 40
        
        # è¦ç‚¹è¦†ç›–
        coverage_score = self.text_analyzer.calculate_coverage(model_answer, reference) * 60
        
        return min(100.0, length_score + coverage_score)
    
    def _evaluate_clarity_fallback(self, model_answer: str, question: Dict) -> float:
        """è¯„ä¼°ç­”æ¡ˆæ¸…æ™°åº¦ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        if not model_answer:
            return 0.0
        
        score = 60.0  # åŸºç¡€åˆ†
        
        # ç»“æ„åŒ–ç¨‹åº¦
        structure_score = self.text_analyzer.evaluate_structure(model_answer)
        score += structure_score * 0.4
        
        # é€»è¾‘è¿è´¯æ€§
        coherence_score = self.text_analyzer.evaluate_coherence(model_answer)
        score += coherence_score * 0.4
        
        # ä¸“ä¸šåº¦
        professionalism_score = self.text_analyzer.evaluate_professionalism(model_answer)
        score += professionalism_score * 0.2
        
        return min(100.0, score)
    
    def _generate_feedback(self, scores: Dict[str, float], answer: str) -> str:
        """ç”Ÿæˆè¯„ä¼°åé¦ˆ"""
        overall_score = scores.get('overall', 0)
        feedback_parts = []
        
        if overall_score >= 80:
            feedback_parts.append("ä¼˜ç§€çš„å›ç­”ï¼")
        elif overall_score >= 60:
            feedback_parts.append("å›ç­”è´¨é‡è‰¯å¥½ã€‚")
        else:
            feedback_parts.append("å›ç­”éœ€è¦æ”¹è¿›ã€‚")
        
        # å…·ä½“å»ºè®®
        if scores.get('accuracy', 0) < 60:
            feedback_parts.append("å‡†ç¡®æ€§éœ€è¦æå‡ï¼Œå»ºè®®æ ¸å®å…³é”®ä¿¡æ¯ã€‚")
        
        if scores.get('completeness', 0) < 60:
            feedback_parts.append("å›ç­”ä¸å¤Ÿå®Œæ•´ï¼Œå»ºè®®è¡¥å……æ›´å¤šç»†èŠ‚ã€‚")
        
        if scores.get('clarity', 0) < 60:
            feedback_parts.append("è¡¨è¾¾ä¸å¤Ÿæ¸…æ™°ï¼Œå»ºè®®æ”¹è¿›ç»“æ„å’Œé€»è¾‘ã€‚")
        
        return " ".join(feedback_parts) 