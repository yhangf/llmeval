#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¼–ç¨‹è¯„ä¼°å™¨æ¨¡å—
ä¸“é—¨å¤„ç†ç¼–ç¨‹é¢˜çš„è¯„ä¼°é€»è¾‘
"""

import asyncio
import json
import re
from typing import Dict, Any
from .evaluation_types import EvaluationResult, EvaluationScores, QuestionType
from .score_calculator import ScoreCalculator


class ProgrammingEvaluator:
    """ç¼–ç¨‹è¯„ä¼°å™¨"""
    
    def __init__(self, prompt_loader):
        self.prompt_loader = prompt_loader
        self.score_calculator = ScoreCalculator()
    
    def should_use_programming_evaluation(self, question: Dict, reference_answer: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨ç¼–ç¨‹è¯„ä¼°"""
        question_category = question.get('category', '').lower()
        question_type = question.get('type', '')
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºç¼–ç¨‹ç±»å‹é—®é¢˜
        is_programming = (
            question_category == 'ç¼–ç¨‹' or 
            question_category == 'ç¼–ç¨‹å®è·µ' or
            'ç¼–ç¨‹' in question_category
        )
        
        # å¦‚æœæ˜¯ç¼–ç¨‹ç±»å‹é—®é¢˜ï¼Œåº”è¯¥ä¼˜å…ˆä½¿ç”¨ç¼–ç¨‹è¯„ä¼°
        should_use_programming_eval = is_programming
        
        return should_use_programming_eval
    
    async def evaluate_programming_response(self, question_data: dict, model_answer: str,
                                          standard_answer: str, evaluator_model, logger=None) -> Dict[str, Any]:
        """ä½¿ç”¨è¯„ä¼°æ¨¡å‹è¯„ä¼°ç¼–ç¨‹é¢˜"""
        question_type = question_data.get('type', 'standard_answer')
        
        # åˆ›å»ºç¼–ç¨‹è¯„ä¼°æç¤º
        prompt = self.prompt_loader.create_programming_evaluation_prompt(
            question_data, model_answer, standard_answer, question_type
        )
        
        print(f"ğŸ¯ ä½¿ç”¨ç¼–ç¨‹è¯„ä¼°æ¨¡æ¿ - é—®é¢˜ID: {question_data.get('id')}, ç±»å‹: {question_type}")
        
        try:
            await asyncio.sleep(1)  # é¿å…APIé™åˆ¶
            
            # è®°å½•è¯„ä¼°æ¨¡å‹è¯·æ±‚
            if logger:
                logger.log_model_request(evaluator_model.__class__.__name__, prompt, 
                                       {"max_tokens": 500, "temperature": 0.1})
            
            response = await evaluator_model.generate(prompt, max_tokens=5000, temperature=0.1)
            
            if response.get('error'):
                raise Exception(response['error'])
            
            content = response.get('content', '').strip()
            print(f"ç¼–ç¨‹é¢˜è¯„ä¼°ç»“æœ: {content}")
            
            # è®°å½•è¯„ä¼°æ¨¡å‹å›ç­”
            if logger:
                logger.log_model_response(evaluator_model.__class__.__name__, response, "ç¼–ç¨‹è¯„ä¼°ç»“æœ")
            
            # è§£æJSONæ ¼å¼çš„è¯„ä¼°ç»“æœ
            try:
                eval_result = self._extract_json_from_text(content)
                if not eval_result:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œä½¿ç”¨é»˜è®¤è§£æ
                    eval_result = self._parse_evaluation_response(content)
                
                # è®¡ç®—æ€»åˆ†
                total_score = self.score_calculator.calculate_programming_score(question_data, eval_result)
                
                return {
                    'scores': {
                        'accuracy': eval_result.get('accuracy', 0),
                        'completeness': eval_result.get('completeness', 0),
                        'clarity': eval_result.get('clarity', 0),
                        'overall': total_score
                    },
                    'requirement_completed': eval_result.get('requirement_completed', False),
                    'sub_question_scores': eval_result.get('sub_question_scores', []),
                    'feedback': eval_result.get('feedback', 'æ— è¯¦ç»†åé¦ˆ'),
                    'tokens_used': response.get('usage', {}).get('total_tokens', 0)
                }
                
            except (json.JSONDecodeError, KeyError) as e:
                print(f"è§£æè¯„ä¼°ç»“æœå¤±è´¥: {e}")
                print(f"åŸå§‹è¯„ä¼°å†…å®¹: {content}")
                # ä½¿ç”¨å¤‡ç”¨è§£ææ–¹æ³•
                return self._parse_evaluation_response(content)
                
        except Exception as e:
            print(f"ç¼–ç¨‹è¯„ä¼°å¤±è´¥ï¼Œå›é€€åˆ°é€šç”¨è¯„ä¼°: {e}")
            # è¿™é‡Œä¸è¿”å›é”™è¯¯ï¼Œè€Œæ˜¯æŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚å¤„ç†
            raise e
    
    def extract_answer_from_response(self, model_answer: str) -> str:
        """ä»æ¨¡å‹å›ç­”ä¸­æå–ç­”æ¡ˆéƒ¨åˆ†"""
        # é¦–å…ˆå°è¯•æå–<answer>æ ‡ç­¾ä¸­çš„å†…å®¹
        answer_match = re.search(r'<answer>(.*?)</answer>', model_answer, re.DOTALL)
        if answer_match:
            return answer_match.group(1).strip()
        
        # å¦‚æœæ²¡æœ‰answeræ ‡ç­¾ï¼Œå°è¯•æå–model_answeræ ‡ç­¾ä¸­çš„å†…å®¹
        code_match = re.search(r'<model_answer>(.*?)</model_answer>', model_answer, re.DOTALL)
        if code_match:
            return code_match.group(1).strip()
        
        # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œä½¿ç”¨å…¨éƒ¨å†…å®¹
        return model_answer
    
    def _extract_json_from_text(self, text: str) -> Dict[str, Any]:
        """ä»æ–‡æœ¬ä¸­æå–JSONå¯¹è±¡"""
        # æ–¹æ³•1ï¼šå¯»æ‰¾å®Œæ•´çš„JSONå¯¹è±¡ï¼ˆæ”¯æŒåµŒå¥—ï¼‰
        brace_count = 0
        start_idx = -1
        
        for i, char in enumerate(text):
            if char == '{':
                if brace_count == 0:
                    start_idx = i
                brace_count += 1
            elif char == '}':
                brace_count -= 1
                if brace_count == 0 and start_idx != -1:
                    try:
                        json_str = text[start_idx:i+1]
                        return json.loads(json_str)
                    except json.JSONDecodeError:
                        continue
        
        # æ–¹æ³•2ï¼šä½¿ç”¨æ›´å®½æ¾çš„æ­£åˆ™è¡¨è¾¾å¼
        patterns = [
            r'\{[^{}]*"requirement_completed"[^{}]*\}',  # ç®€å•JSON
            r'\{.*?"requirement_completed".*?\}',       # åŒ…å«æ¢è¡Œçš„JSON
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group())
                except json.JSONDecodeError:
                    continue
        
        return None
    
    def _parse_evaluation_response(self, response: str) -> Dict[str, Any]:
        """è§£æè¯„ä¼°å“åº”ï¼Œæå–åˆ†æ•°å’Œåé¦ˆ"""
        # å°è¯•æå–åˆ†æ•°ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        def extract_score(pattern, default=50):
            match = re.search(pattern, response)
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    return default
            return default
        
        accuracy = extract_score(r'å‡†ç¡®æ€§[ï¼š:]\s*(\d+)')
        completeness = extract_score(r'å®Œæ•´æ€§[ï¼š:]\s*(\d+)')
        clarity = extract_score(r'æ¸…æ™°åº¦[ï¼š:]\s*(\d+)')
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆéœ€æ±‚ - å¢åŠ æ›´å¤šå…³é”®è¯
        requirement_keywords = ['å®Œæˆ', 'true', 'æ­£ç¡®', 'æˆåŠŸ', 'è¾¾æˆ', 'æ»¡è¶³', 'ç¬¦åˆ', 'é€šè¿‡']
        not_requirement_keywords = ['æœªå®Œæˆ', 'false', 'é”™è¯¯', 'å¤±è´¥', 'ä¸ç¬¦åˆ', 'ä¸æ»¡è¶³', 'ä¸é€šè¿‡']
        
        # æ£€æŸ¥å¦å®šå…³é”®è¯
        has_negative = any(keyword in response.lower() for keyword in not_requirement_keywords)
        has_positive = any(keyword in response.lower() for keyword in requirement_keywords)
        
        # å¦‚æœæœ‰æ˜ç¡®çš„å¦å®šï¼Œåˆ™æœªå®Œæˆï¼›å¦åˆ™æ ¹æ®ç§¯æå…³é”®è¯åˆ¤æ–­
        requirement_completed = has_positive and not has_negative
        
        # å°è¯•æå–å­é—®é¢˜åˆ†æ•°
        sub_scores = []
        sub_pattern = r'å­é—®é¢˜\s*(\d+)[ï¼š:]\s*(\d+)'
        sub_matches = re.findall(sub_pattern, response)
        for match in sub_matches:
            try:
                sub_scores.append(int(match[1]))
            except ValueError:
                continue
        
        return {
            'accuracy': accuracy,
            'completeness': completeness,
            'clarity': clarity,
            'requirement_completed': requirement_completed,
            'feedback': response,
            'sub_question_scores': sub_scores
        } 