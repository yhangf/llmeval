#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥å¿—æŸ¥çœ‹å·¥å…·
æä¾›ç®€å•çš„æ—¥å¿—æ–‡ä»¶ç®¡ç†å’ŒæŸ¥çœ‹åŠŸèƒ½
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.evaluation.logger import EvaluationLogger


def list_evaluation_logs():
    """åˆ—å‡ºæ‰€æœ‰è¯„ä¼°æ—¥å¿—æ–‡ä»¶"""
    print("ğŸ“‹ è¯„ä¼°æ—¥å¿—æ–‡ä»¶åˆ—è¡¨:")
    print("=" * 80)
    
    log_files = EvaluationLogger.list_log_files()
    
    if not log_files:
        print("ğŸ“­ æš‚æ— æ—¥å¿—æ–‡ä»¶")
        return
    
    for i, log_file in enumerate(log_files, 1):
        filename = log_file['filename']
        size_kb = log_file['size'] / 1024
        created_time = datetime.fromisoformat(log_file['created_time']).strftime("%Y-%m-%d %H:%M:%S")
        
        # è§£ææ–‡ä»¶åè·å–ä¿¡æ¯
        parts = filename.replace('evaluation_', '').replace('.log', '').split('_')
        if len(parts) >= 3:
            date_time = f"{parts[0]}_{parts[1]}"
            model = parts[2] if len(parts) > 2 else "unknown"
            dataset = '_'.join(parts[3:]) if len(parts) > 3 else "unknown"
            
            print(f"{i:2d}. {filename}")
            print(f"    ğŸ“… æ—¶é—´: {created_time}")
            print(f"    ğŸ¤– æ¨¡å‹: {model}")
            print(f"    ğŸ“Š æ•°æ®é›†: {dataset}")
            print(f"    ğŸ“ å¤§å°: {size_kb:.1f} KB")
            print(f"    ğŸ“ è·¯å¾„: {log_file['path']}")
            
            # æ˜¾ç¤ºJSONæ–‡ä»¶ä¿¡æ¯
            if log_file.get('has_json'):
                json_size_kb = log_file['json_size'] / 1024
                print(f"    ğŸ“„ JSONæ•°æ®: {json_size_kb:.1f} KB ({log_file['json_file']})")
            else:
                print(f"    ğŸ“„ JSONæ•°æ®: æ— ")
            print()


def view_log_file(log_path: str, lines: int = 50):
    """æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶å†…å®¹"""
    if not os.path.exists(log_path):
        print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_path}")
        return
    
    print(f"ğŸ“– æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: {os.path.basename(log_path)}")
    print("=" * 80)
    
    try:
        with open(log_path, 'r', encoding='utf-8') as f:
            content = f.readlines()
        
        total_lines = len(content)
        print(f"ğŸ“„ æ€»è¡Œæ•°: {total_lines}")
        
        if lines == -1:  # æ˜¾ç¤ºå…¨éƒ¨
            lines_to_show = content
            print("ğŸ“º æ˜¾ç¤ºå…¨éƒ¨å†…å®¹:")
        else:
            lines_to_show = content[-lines:] if total_lines > lines else content
            print(f"ğŸ“º æ˜¾ç¤ºæœ€å {len(lines_to_show)} è¡Œ:")
        
        print("-" * 80)
        for line in lines_to_show:
            print(line.rstrip())
            
    except Exception as e:
        print(f"âŒ è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")


def view_json_data(json_path: str, section: str = "all"):
    """æŸ¥çœ‹JSONæ ¼å¼çš„è¯„ä¼°æ•°æ®"""
    if not os.path.exists(json_path):
        print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
        return
    
    print(f"ğŸ“„ æŸ¥çœ‹JSONæ•°æ®: {os.path.basename(json_path)}")
    print("=" * 80)
    
    try:
        data = EvaluationLogger.load_json_data(json_path)
        if not data:
            print("âŒ æ— æ³•åŠ è½½JSONæ•°æ®")
            return
        
        if section == "info" or section == "all":
            print("ğŸ“‹ ä¼šè¯ä¿¡æ¯:")
            session_info = data.get('session_info', {})
            for key, value in session_info.items():
                print(f"  {key}: {value}")
            print()
        
        if section == "questions" or section == "all":
            questions = data.get('questions_and_answers', [])
            print(f"ğŸ“ é—®é¢˜å’Œå›ç­” ({len(questions)} ä¸ª):")
            
            for i, q in enumerate(questions, 1):
                print(f"\n{i}. é—®é¢˜ID: {q.get('question_id')}")
                print(f"   é—®é¢˜å†…å®¹: {q.get('question_content', '')[:100]}...")
                
                # æ˜¾ç¤ºæ¨¡å‹äº¤äº’
                interactions = q.get('model_interactions', [])
                print(f"   æ¨¡å‹äº¤äº’: {len(interactions)} æ¬¡")
                
                for interaction in interactions:
                    if interaction['type'] == 'request':
                        print(f"     ğŸ”µ è¯·æ±‚ -> {interaction['model_name']}")
                        print(f"        æç¤ºè¯é•¿åº¦: {interaction['prompt_length']} å­—ç¬¦")
                    elif interaction['type'] == 'response':
                        print(f"     ğŸŸ¢ å“åº” <- {interaction['model_name']}")
                        content = interaction.get('content', '')
                        print(f"        å›ç­”é•¿åº¦: {len(content)} å­—ç¬¦")
                        print(f"        Tokenä½¿ç”¨: {interaction.get('tokens_used', 0)}")
                        if interaction.get('error'):
                            print(f"        âŒ é”™è¯¯: {interaction['error']}")
                
                # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
                evaluation = q.get('evaluation_result', {})
                if evaluation:
                    scores = evaluation.get('scores', {})
                    print(f"   ğŸ“Š è¯„ä¼°ç»“æœ:")
                    print(f"        æ€»åˆ†: {scores.get('overall', 0):.1f}")
                    print(f"        å‡†ç¡®æ€§: {scores.get('accuracy', 0):.1f}")
                    print(f"        å®Œæ•´æ€§: {scores.get('completeness', 0):.1f}")
                    print(f"        æ¸…æ™°åº¦: {scores.get('clarity', 0):.1f}")
                    
                    if evaluation.get('feedback'):
                        print(f"        åé¦ˆ: {evaluation['feedback'][:100]}...")
        
        if section == "summary" or section == "all":
            print("\nğŸ“ˆ ä¼šè¯æ€»ç»“:")
            summary = data.get('session_summary', {})
            for key, value in summary.items():
                if key == 'score_statistics':
                    print(f"  {key}:")
                    for metric, stats in value.items():
                        print(f"    {metric}: å¹³å‡{stats.get('mean', 0):.1f} æœ€é«˜{stats.get('max', 0):.1f} æœ€ä½{stats.get('min', 0):.1f}")
                else:
                    print(f"  {key}: {value}")
        
        file_size = os.path.getsize(json_path) / 1024
        print(f"\nğŸ“ æ–‡ä»¶å¤§å°: {file_size:.1f} KB")
        
    except Exception as e:
        print(f"âŒ è¯»å–JSONæ•°æ®å¤±è´¥: {e}")


def export_qa_pairs(json_path: str, output_file: str = None):
    """å¯¼å‡ºé—®ç­”å¯¹ä¸ºç®€åŒ–çš„JSONæ ¼å¼"""
    if not os.path.exists(json_path):
        print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
        return
    
    try:
        data = EvaluationLogger.load_json_data(json_path)
        if not data:
            print("âŒ æ— æ³•åŠ è½½JSONæ•°æ®")
            return
        
        # æå–é—®ç­”å¯¹
        qa_pairs = []
        questions = data.get('questions_and_answers', [])
        
        for q in questions:
            # è·å–é—®é¢˜
            question_data = {
                "question_id": q.get('question_id'),
                "question": q.get('question_content', ''),
                "full_question": q.get('full_question_data', {}),
                "model_responses": [],
                "evaluation": q.get('evaluation_result', {})
            }
            
            # è·å–æ¨¡å‹å›ç­”
            interactions = q.get('model_interactions', [])
            current_request = None
            
            for interaction in interactions:
                if interaction['type'] == 'request':
                    current_request = interaction
                elif interaction['type'] == 'response' and current_request:
                    response_data = {
                        "model_name": interaction['model_name'],
                        "request": {
                            "prompt": current_request.get('prompt', ''),
                            "config": current_request.get('config', {}),
                            "timestamp": current_request.get('timestamp', '')
                        },
                        "response": {
                            "content": interaction.get('content', ''),
                            "tokens_used": interaction.get('tokens_used', 0),
                            "timestamp": interaction.get('timestamp', ''),
                            "error": interaction.get('error')
                        }
                    }
                    question_data["model_responses"].append(response_data)
                    current_request = None
            
            qa_pairs.append(question_data)
        
        # åˆ›å»ºå¯¼å‡ºæ•°æ®
        export_data = {
            "session_info": data.get('session_info', {}),
            "qa_pairs": qa_pairs,
            "summary": data.get('session_summary', {}),
            "export_time": datetime.now().isoformat()
        }
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
        if not output_file:
            session_id = data.get('session_info', {}).get('session_id', 'unknown')
            output_file = f"qa_export_{session_id}.json"
        
        # ä¿å­˜æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… é—®ç­”å¯¹å·²å¯¼å‡ºåˆ°: {output_file}")
        print(f"ğŸ“Š å¯¼å‡ºç»Ÿè®¡:")
        print(f"   é—®é¢˜æ•°é‡: {len(qa_pairs)}")
        
        total_responses = sum(len(q['model_responses']) for q in qa_pairs)
        print(f"   å›ç­”æ•°é‡: {total_responses}")
        
        file_size = os.path.getsize(output_file) / 1024
        print(f"   æ–‡ä»¶å¤§å°: {file_size:.1f} KB")
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")


def search_logs(keyword: str):
    """åœ¨æ—¥å¿—æ–‡ä»¶ä¸­æœç´¢å…³é”®è¯"""
    print(f"ğŸ” åœ¨æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ä¸­æœç´¢å…³é”®è¯: '{keyword}'")
    print("=" * 80)
    
    log_files = EvaluationLogger.list_log_files()
    found_results = []
    
    for log_file in log_files:
        log_path = log_file['path']
        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            for line_num, line in enumerate(lines, 1):
                if keyword.lower() in line.lower():
                    found_results.append({
                        'file': os.path.basename(log_path),
                        'line_num': line_num,
                        'content': line.strip()
                    })
        except Exception as e:
            print(f"âŒ æœç´¢æ–‡ä»¶ {log_path} æ—¶å‡ºé”™: {e}")
    
    if found_results:
        print(f"âœ… æ‰¾åˆ° {len(found_results)} ä¸ªåŒ¹é…ç»“æœ:")
        print()
        
        for result in found_results:
            print(f"ğŸ“ {result['file']} (ç¬¬{result['line_num']}è¡Œ)")
            print(f"   {result['content']}")
            print()
    else:
        print(f"âŒ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ '{keyword}' çš„æ—¥å¿—æ¡ç›®")


def cleanup_old_logs(days: int = 7):
    """æ¸…ç†Nå¤©å‰çš„æ—§æ—¥å¿—æ–‡ä»¶"""
    print(f"ğŸ§¹ æ¸…ç† {days} å¤©å‰çš„æ—§æ—¥å¿—æ–‡ä»¶...")
    
    log_files = EvaluationLogger.list_log_files()
    current_time = datetime.now()
    deleted_count = 0
    
    for log_file in log_files:
        created_time = datetime.fromisoformat(log_file['created_time'])
        age_days = (current_time - created_time).days
        
        if age_days > days:
            try:
                os.remove(log_file['path'])
                print(f"ğŸ—‘ï¸  åˆ é™¤: {log_file['filename']} (åˆ›å»ºäº {age_days} å¤©å‰)")
                deleted_count += 1
                
                # åˆ é™¤å¯¹åº”çš„JSONæ–‡ä»¶
                if log_file.get('has_json'):
                    os.remove(log_file['json_file'])
                    print(f"ğŸ—‘ï¸  åˆ é™¤JSON: {os.path.basename(log_file['json_file'])}")
                    
            except Exception as e:
                print(f"âŒ åˆ é™¤å¤±è´¥: {log_file['filename']} - {e}")
    
    if deleted_count > 0:
        print(f"âœ… æˆåŠŸåˆ é™¤ {deleted_count} ä¸ªæ—§æ—¥å¿—æ–‡ä»¶")
    else:
        print("ğŸ“­ æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—§æ—¥å¿—æ–‡ä»¶")

def cleanup_old_task_logs(max_tasks: int = 5):
    """æ¸…ç†æ—§ä»»åŠ¡çš„æ—¥å¿—æ–‡ä»¶ï¼Œåªä¿ç•™æœ€è¿‘Nä¸ªä»»åŠ¡çš„æ—¥å¿—"""
    try:
        print(f"ğŸ§¹ è‡ªåŠ¨æ¸…ç†æ—§ä»»åŠ¡æ—¥å¿—ï¼Œä¿ç•™æœ€è¿‘ {max_tasks} ä¸ªä»»åŠ¡çš„æ—¥å¿—...")
        
        log_files = EvaluationLogger.list_log_files()
        
        if len(log_files) <= max_tasks:
            print("ğŸ“­ æ—¥å¿—æ–‡ä»¶æ•°é‡æœªè¶…è¿‡é™åˆ¶ï¼Œæ— éœ€æ¸…ç†")
            return
        
        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åº
        log_files.sort(key=lambda x: x['created_time'], reverse=True)
        
        # ä¿ç•™æœ€è¿‘çš„max_tasksä¸ªæ—¥å¿—ï¼Œåˆ é™¤å…¶ä½™çš„
        logs_to_keep = log_files[:max_tasks]
        logs_to_delete = log_files[max_tasks:]
        
        deleted_count = 0
        for log_file in logs_to_delete:
            try:
                # åˆ é™¤æ—¥å¿—æ–‡ä»¶
                os.remove(log_file['path'])
                deleted_count += 1
                print(f"ğŸ—‘ï¸  åˆ é™¤æ—§æ—¥å¿—: {log_file['filename']}")
                
                # åˆ é™¤å¯¹åº”çš„JSONæ–‡ä»¶
                if log_file.get('has_json'):
                    os.remove(log_file['json_file'])
                    print(f"ğŸ—‘ï¸  åˆ é™¤å¯¹åº”JSON: {os.path.basename(log_file['json_file'])}")
                    
            except Exception as e:
                print(f"âŒ åˆ é™¤æ—¥å¿—å¤±è´¥: {log_file['filename']} - {e}")
        
        if deleted_count > 0:
            print(f"âœ… è‡ªåŠ¨æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ—§ä»»åŠ¡æ—¥å¿—ï¼Œä¿ç•™æœ€è¿‘çš„ {max_tasks} ä¸ªä»»åŠ¡æ—¥å¿—")
        else:
            print("ğŸ“­ æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—§ä»»åŠ¡æ—¥å¿—")
            
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨æ¸…ç†ä»»åŠ¡æ—¥å¿—å¤±è´¥: {e}")


def show_log_stats():
    """æ˜¾ç¤ºæ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
    print("ğŸ“Š æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯:")
    print("=" * 80)
    
    log_files = EvaluationLogger.list_log_files()
    
    if not log_files:
        print("ğŸ“­ æš‚æ— æ—¥å¿—æ–‡ä»¶")
        return
    
    total_files = len(log_files)
    total_size = sum(f['size'] for f in log_files)
    total_json_files = sum(1 for f in log_files if f.get('has_json'))
    total_json_size = sum(f.get('json_size', 0) for f in log_files if f.get('has_json'))
    
    # æŒ‰æ¨¡å‹ç»Ÿè®¡
    models = {}
    datasets = {}
    
    for log_file in log_files:
        filename = log_file['filename']
        parts = filename.replace('evaluation_', '').replace('.log', '').split('_')
        
        if len(parts) >= 3:
            model = parts[2]
            dataset = '_'.join(parts[3:]) if len(parts) > 3 else "unknown"
            
            models[model] = models.get(model, 0) + 1
            datasets[dataset] = datasets.get(dataset, 0) + 1
    
    print(f"ğŸ“ æ€»æ—¥å¿—æ–‡ä»¶: {total_files}")
    print(f"ğŸ“„ JSONæ•°æ®æ–‡ä»¶: {total_json_files}")
    print(f"ğŸ’¾ æ€»å¤§å°: {(total_size + total_json_size) / 1024:.1f} KB")
    print(f"   æ—¥å¿—: {total_size / 1024:.1f} KB")
    print(f"   JSON: {total_json_size / 1024:.1f} KB")
    print()
    
    print("ğŸ¤– æŒ‰æ¨¡å‹ç»Ÿè®¡:")
    for model, count in sorted(models.items()):
        print(f"   {model}: {count} æ¬¡è¯„ä¼°")
    
    print()
    print("ğŸ“Š æŒ‰æ•°æ®é›†ç»Ÿè®¡:")
    for dataset, count in sorted(datasets.items()):
        print(f"   {dataset}: {count} æ¬¡è¯„ä¼°")


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ğŸ“‹ æ—¥å¿—æŸ¥çœ‹å·¥å…·ä½¿ç”¨è¯´æ˜:")
        print("python utils/log_viewer.py <command> [options]")
        print()
        print("å¯ç”¨å‘½ä»¤:")
        print("  list                        - åˆ—å‡ºæ‰€æœ‰æ—¥å¿—æ–‡ä»¶")
        print("  view <file_path> [n]        - æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ (å¯é€‰: æ˜¾ç¤ºè¡Œæ•°, -1è¡¨ç¤ºå…¨éƒ¨)")
        print("  json <json_path> [section]  - æŸ¥çœ‹JSONæ•°æ® (section: info/questions/summary/all)")
        print("  export <json_path> [output] - å¯¼å‡ºé—®ç­”å¯¹ä¸ºç®€åŒ–JSONæ ¼å¼")
        print("  search <keyword>            - æœç´¢å…³é”®è¯")
        print("  stats                       - æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯")
        print("  cleanup [days]              - æ¸…ç†æ—§æ—¥å¿— (é»˜è®¤7å¤©)")
        print()
        print("ç¤ºä¾‹:")
        print("  python utils/log_viewer.py list")
        print("  python utils/log_viewer.py view logs/evaluation_xxx.log 100")
        print("  python utils/log_viewer.py json logs/evaluation_xxx.json questions")
        print("  python utils/log_viewer.py export logs/evaluation_xxx.json qa_output.json")
        print("  python utils/log_viewer.py search 'é”™è¯¯'")
        print("  python utils/log_viewer.py cleanup 7")
        return
    
    command = sys.argv[1].lower()
    
    if command == 'list':
        list_evaluation_logs()
    
    elif command == 'view':
        if len(sys.argv) < 3:
            print("âŒ è¯·æŒ‡å®šè¦æŸ¥çœ‹çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„")
            return
        
        log_path = sys.argv[2]
        lines = int(sys.argv[3]) if len(sys.argv) > 3 else 50
        view_log_file(log_path, lines)
    
    elif command == 'json':
        if len(sys.argv) < 3:
            print("âŒ è¯·æŒ‡å®šè¦æŸ¥çœ‹çš„JSONæ–‡ä»¶è·¯å¾„")
            return
        
        json_path = sys.argv[2]
        section = sys.argv[3] if len(sys.argv) > 3 else "all"
        view_json_data(json_path, section)
    
    elif command == 'export':
        if len(sys.argv) < 3:
            print("âŒ è¯·æŒ‡å®šè¦å¯¼å‡ºçš„JSONæ–‡ä»¶è·¯å¾„")
            return
        
        json_path = sys.argv[2]
        output_file = sys.argv[3] if len(sys.argv) > 3 else None
        export_qa_pairs(json_path, output_file)
    
    elif command == 'search':
        if len(sys.argv) < 3:
            print("âŒ è¯·æŒ‡å®šæœç´¢å…³é”®è¯")
            return
        
        keyword = sys.argv[2]
        search_logs(keyword)
    
    elif command == 'stats':
        show_log_stats()
    
    elif command == 'cleanup':
        days = int(sys.argv[2]) if len(sys.argv) > 2 else 7
        cleanup_old_logs(days)
    
    else:
        print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
        print("ä½¿ç”¨ 'python utils/log_viewer.py' æŸ¥çœ‹å¸®åŠ©")


if __name__ == "__main__":
    main() 